# Proposal 002 - Automatically re-load proxy pods on Kubernetes config change 

<!-- TOC -->
* [Proposal 002 - Automatically re-load proxy pods on Kubernetes config change](#proposal-002---automatically-re-load-proxy-pods-on-kubernetes-config-change-)
* [Problem Statement](#problem-statement)
* [Proposed Solution](#proposed-solution)
  * [The details](#the-details)
  * [Advantages](#advantages)
  * [Limitations](#limitations)
* [Rejected Alternatives](#rejected-alternatives)
<!-- TOC -->
# Problem Statement
There are a wide array of Kubernetes resources which are used to create the configuration used by any given Kroxylicious proxy process. These resources are consumed and generated by multiple independent reconcilers (in Java Operator SDK terms). Which leads to a tangled web of resources, reconcilers and triggers. 

We split the operator into multiple reconcilers so that there was a single writer for each of our "intermediate" resources, and thus it is much easier to reason about what changed and why. However, that means that we do not have a single global view of when a proxies configuration needs re-generated.

# Proposed Solution

The Kroxylicious operator is built up from what we refer to as "leaf" and "aggregating" reconcilers. 
We propose that each leaf reconciler should generate a hash of all the resources it has consumed to generate the required config for its reconciliation and include that in its status. The aggregate reconcilers would then be responsible for computing the ultimate hash representing the hash of all the configuration in use in a given proxy pod. The aggregating reconciler will then annotate the deployment with the config hash, thus when any node in the config model or external resource is modified the deployment is updated.    

## The details

| Reconciler            | Roles            | Aggregated by         | Watches resources                      |
|-----------------------|------------------|-----------------------|----------------------------------------|
| Kafka Proxy           | aggregator       | not aggregated        | Virtual Kafka Cluster CRs              |
| Virtual Kafka Cluster | aggregator, leaf | Kafka Proxy           | Kafka Protocol Filters, Kafka Services |
| Kafka Protocol Filter | leaf             | Virtual Kafka Cluster |                                        |
| Kafka Service         | leaf             | Virtual Kafka Cluster |                                        |
| Kafka Proxy Ingress   | leaf             | Kafka Proxy Cluster   |                                        |


## Advantages
Maintains the same logical division between reconcilers thus avoiding the need for the aggregating reconciler to watch all resources involved and thus the need for it to understand the details of the full configuration model and its kubernetes representation. 

## Limitations
1. The blast radius of any config change is still the whole proxy process. I.e. a malformed config file still prevents the whole proxy from starting.
This is largely mitigated by the fact the config file is in the kubernetes case is programmatically generated, and we can validate that before applying it.
2. Any change to the config model requires the entire proxy process to restart. I.e. renewing the certificate of one virtual cluster requires the all connections to the hosting proxy instance to be disconnected and reconnect.

# Rejected Alternatives
1. Have the Kafka Proxy reconciler (KPR) watch all resources involved for changes and trigger on any update. This has a variety of issues
   1. We conflate concerns between reconcilers as we end up with two (or more) reconcilers watching the same resource.
   2. The KPR can, and likely will, be triggered on invalid or intermediate states thus making for vastly more complicated error handling and recovery
   3. It becomes much harder for operators to work out what triggered a particular change to the running processes.
2. The proxy process monitors the config file(s) for changes 
   1. What set of files should the proxy monitor? The set can grow and shrink during parsing as filers are added and removed
   2. How does the proxy know when a file is fully updated? User saves changes to the live config file before looking up the correct config syntax.
   3. We add complexity to the proxy, extra threads for file watches.
3. Follow the model pioneered by HTTPD but copied by many other services which support vhosting of a `conf.d` directory and have one file per virtual cluster.
   1. Increases the complexity of managing bare metal clusters 
   2. Makes crafting valid config harder (identifying for port collisions across a directory of file)
   3. Increases complexity within the proxy
   4. Lack of developer bandwidth to context switch to the proxy
