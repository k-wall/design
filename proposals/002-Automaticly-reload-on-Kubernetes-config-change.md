# Proposal 002 - Automatically trigger a deployment rollout of affected proxies on Kubernetes config change 

<!-- TOC -->
* [Proposal 002 - Automatically trigger a deployment rollout of affected proxies on Kubernetes config change](#proposal-002---automatically-trigger-a-deployment-rollout-of-affected-proxies-on-kubernetes-config-change-)
* [Problem Statement](#problem-statement)
* [Proposed Solution](#proposed-solution)
  * [The details](#the-details)
  * [Advantages](#advantages)
  * [Limitations](#limitations)
* [Rejected Alternatives](#rejected-alternatives)
<!-- TOC -->
# Problem Statement
There are a wide array of Kubernetes resources which are used to create the configuration used by any given Kroxylicious proxy process. These resources are consumed and generated by multiple independent reconcilers (in Java Operator SDK terms). Which leads to a tangled web of resources, reconcilers and triggers. 

We decided to split the operator into multiple reconcilers so that there was a single writer for each of our "intermediate" resources (KafkaService, VirtualKafkaCluster, etc), and thus it is much easier to reason about what changed and why. However, that means that we do not have a single global view of when a proxy's configuration needs to be re-generated.

# Proposed Solution

The Kroxylicious operator is built up from what we refer to as "leaf" (such as the Kafka Service Reconciler) and "aggregating" (with the Kafka Proxy Reconciler being the ultimate aggregator) reconcilers. A leaf reconciler is one that consumes a single instance of a Kroxylicious CRD and determines its validity. An aggregating reconciler on the other hand operates on a collection of Kroxylicious CRDs which are "managed" by other reconcilers and uses those to determine the desired state of the Kroxylicious CRD it is responsible for.
We propose that each leaf reconciler should generate a hash of all the resources it has consumed to generate the required config for its reconciliation and include that in its status. The aggregate reconcilers would then be responsible for computing the ultimate hash representing the hash of all the configuration in use in a given proxy pod. The aggregating reconciler will then annotate the deployment with the config hash, thus when any node in the config model or external resource is modified the deployment is updated.    

## The details

Each leaf reconciler will resolve references to the external resources it is aware of and compute a checksum[^1] derived from the metadata uid & generation of each resource. The checksum will be added as an annotation (`kroxylicious.io/dependants-checksum`) of the Reconcilers resource.
The aggregating reconciler will consume the checksums from each leaf reconciler and compute its own checksum combining the uid, generation and status checksum of each resource its aggregating.
The Kafka Proxy Reconciler, KPR, will compute the ultimate checksum and include it as an annotation in the deployment's `spec.template`.
This means that each reconciler is responsible for calculating the checksum only of its own external resources but not of its primary resource.

By including the checksum in the pod template it means that everytime a part of the configuration model for a proxy instance is updated the pods will be replaced as their annotations no longer match.  

The reconciler will only include/update the referent-checksum when it considers the result to be successful. This is because the changing the checksum applied to the deployment is intended to trigger a deployment rollout, and we only want to do that when the configuration is in a valid state. 

|      Reconciler       |      Roles       |     Aggregated by     |                                Watches resources                                 |
|:---------------------:|:----------------:|:---------------------:|:--------------------------------------------------------------------------------:|
|      Kafka Proxy      |    aggregator    |    not aggregated     |                       Virtual Kafka Cluster CR, config map                       |
| Virtual Kafka Cluster | aggregator, leaf |      Kafka Proxy      | Virtual Kafka Cluster CR, Kafka Protocol Filter CR, Kafka Service CR, config map |
| Kafka Protocol Filter |       leaf       | Virtual Kafka Cluster |                             Kafka Protocol Filter CR                             |
|     Kafka Service     |       leaf       | Virtual Kafka Cluster |                       Kafka Service CR, Kubernetes Secrets                       |
|  Kafka Proxy Ingress  |       leaf       |  Kafka Proxy Cluster  |                              Kafka Proxy Ingress CR                              |

To illustrate
The Kafka Service Reconciler, KSR, will: 
1. Watch for Kafka Service CRs to appear
2. Parse the KafkaService
   1. Follow any references to external secretes
   2. For each valid secret
      1. Compute a checksum of secret UID & Generation
3. Annotate the KafkaService with `kroxylicious.io/referent-checksum = <computedChecksum>`  


[^1]: For simplicity CRC32 as that's already supported by the JDK. However, the algorithm used is immaterial.

## Advantages
Maintains the same logical division between reconcilers thus avoiding the need for the aggregating reconciler to watch all resources involved and thus the need for it to understand the details of the full configuration model and its kubernetes representation. Brings a measure of consolidation to updates as checksums will only change once per-reconciler pass regardless of the number of resources changed in that pass. 

## Limitations
1. The blast radius of any config change is still the whole proxy process. I.e. a malformed config file still prevents the whole proxy from starting.
This is largely mitigated by the fact the config file is in the kubernetes case is programmatically generated, and we can validate that before applying it.
2. Any change to the config model requires the entire proxy process to restart. I.e. renewing the certificate of one virtual cluster requires the all connections to the hosting proxy instance to be disconnected and reconnect.
3. The operator requires permissions to actually get all the resources (in order to obtain the `metadata.generation` and `metadata.uid`). While it can just query for the specific metadata the Kubernetes RBAC model works at verb/resource level not to fields within resources.
4. This Operator based approach doesn't remove the need for all file watching. When using the secretes store CSI driver with [auto rotation]( https://secrets-store-csi-driver.sigs.k8s.io/topics/secret-auto-rotation) enabled the proxy process would still be required to watch for the secretes being updated.  

# Rejected Alternatives
1. Have the Kafka Proxy reconciler (KPR) watch all resources involved for changes and trigger on any update. This has a variety of issues
   1. We conflate concerns between reconcilers as we end up with two (or more) reconcilers watching the same resource.
   2. The KPR can, and likely will, be triggered on invalid or intermediate states thus making for vastly more complicated error handling and recovery
   3. It becomes much harder for operators to work out what triggered a particular change to the running processes.
2. The proxy process monitors the config file(s) for changes 
   1. What set of files should the proxy monitor? The set can grow and shrink during parsing as filers are added and removed
   2. How does the proxy know when a file is fully updated? User saves changes to the live config file before looking up the correct config syntax.
   3. We add complexity to the proxy, extra threads for file watches.
   4. There is no mechanism for the proxy to report errors back to the operator. e.g. conflicting port allocations. 
3. Follow the model pioneered by HTTPD but copied by many other services which support vhosting of a `conf.d` directory and have one file per virtual cluster.
   1. Increases the complexity of managing bare metal clusters 
   2. Makes crafting valid config harder (identifying for port collisions across a directory of file)
   3. Increases complexity within the proxy
   4. No mechanism to report problems encountered at the proxy level back to the operator.
   5. This solves the orthogonal problem of controlling the blast radius of a configuration change it doesn't really address reloading
